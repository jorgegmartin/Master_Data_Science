{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado de texto con NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qGxsfmzjGyx2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Importamos y descargamos algunos paquetes necesarios\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.tree import Tree\n",
    "from IPython.display import display\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random                              # pseudo-random number generator\n",
    "\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "91_RqhAcE6Te"
   },
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SwgpP6tIE6Tg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n",
      "\n",
      "The type of all_positive_tweets is:  <class 'list'>\n",
      "The type of a tweet entry is:  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))\n",
    "\n",
    "print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n",
    "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets sin preprocesar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echamos un vistazo a los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m@UKBusinessLunch Hi we will be joining you again today :)\n",
      "\u001b[91mKafi din Bad mene aj koi post share ki he so friends like &amp; rewert to bunta he Warnaaaaaaa...........:(\n"
     ]
    }
   ],
   "source": [
    "# print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n"
     ]
    }
   ],
   "source": [
    "# Our selected sample. Complex enough to exemplify each step\n",
    "tweet = all_positive_tweets[2277]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que vienen con cosas como hasthtags, URLs... Deberemos eliminar estas partes en nuestro codigo para poder procesar correctamente el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "\u001b[94m\n",
      "My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n"
     ]
    }
   ],
   "source": [
    "# Eliminamos los enlaces, textos innecesarios como 'RT' y simbolos como '#'\n",
    "# hacemos esto mediante expresiones regulares.\n",
    "\n",
    "import re                                  # library for regular expression operations\n",
    "\n",
    "print('\\033[92m' + tweet)\n",
    "print('\\033[94m')\n",
    "\n",
    "# remove old style retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "# remove hyperlinks\n",
    "tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "\n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "print(tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncTfYM_IaVqB"
   },
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_bdiWMCFi37"
   },
   "source": [
    "La segmentación o **\"tokenización\"** de texto es una de las herramientas fundamentales para preprocesar textos de lenguaje natural. En este notebook vamos a ver algunas de las aproximaciones mas comunes, utilizando en este caso el lenguaje inglés como ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McbHI1XPYaRe"
   },
   "source": [
    "Una de las maneras mas sencillas de realizar esta tarea es segmentar los tokens (palabras) separados por espacios en blanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Om8iVs02G0PO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', \"Andrew's\", 'text,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is Andrew's text, isn't it?\"\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - La palabra it? es en realidad una palabra con un simbolo\n",
    "> - La palabra isn't es en realidad una combinacion semantica de is + not\n",
    "\n",
    "Deberemos utilizar algo un poco más elaborado que utilizar espacios unicamente a la hora de tokenizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'Andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']\n",
      "['This', 'is', 'Andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(text))\n",
    "# el mismo método se puede invocar con la funcion word_tokenize()\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependiendo del idioma, este proceso puede no ser evidente, ni codificable según reglas fijas. Por ejemplo en el caso de idiomas con abundancia de palabras compuestas, como el alemán. \n",
    "\n",
    "Rechtsschutzversicherungsgesellschaften: compañía de seguros que proporciona protección legal \n",
    "\n",
    "Fijémonos que la segmentación no es correcta, incluso itentando utilizar reglas aplicadas al idioma aleman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
